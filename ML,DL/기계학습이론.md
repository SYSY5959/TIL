## 배경

### Loss Func
- 회귀: MSE
- 분류: Cross Entropy
→ loss func 최적화!! convex
	- 함수 간단(convex) : 편미분해서 0 인 값 찾기
	- 함수 convex X (복잡) : Gradient Descent

#### 시간복잡도
`O(1)` < `O(n)` < `O(logn)` < `O(nlogn)` < `O(n²)` < `O(N!)`


## 선형 회귀
- 1개 이상의 독립적인 input variable, 1개의 output target variable
- Cost func:  (LSE) 잔차 제곱합 최소화 → 최적화하는 베타 찾기. 

규제 선형회귀 : 오버핏팅 방지 
[릿지]
- L2 norm : 제곱합
- features 버리기 불가능
- Analytic sol 존재 (손실함수를 베타에 대해 미분 해서 0 되는 베타 찾기 가능 )

[라쏘]
- L1 norm : 절댓값 합
- feature 버리기 가능
- Analytic sol 존재 X → Gradient Descent

## Gradient Descent
- Gradient: func이 가장 빠르게(Duf 최대가 되려면, cos세타 = 1. 세타 = 0 → 같은 방향 ) 증가하는(+ → 오른쪽으로 가야 함수값 증가. - → 왼쪽으로 가야 함수값 증가) 방향 
- G.D : gradient의 반대 방향으로 조금씩 움직여 local minimum 찾는 최적화 알고리즘
→ 모든 train set에 대한 iteration → 비용. 시간 증가

#### Stochastic G.D
- 랜덤으로 뽑은 하나의 데이터에 대한 편미분 값만 계산
→ 빠르지만 Unstable
#### SGD with Minibatch
-랜덤 샘플 M개. 

## Logistic Regression

- 이진 분류 문제에 적합하여, 특정 클래스에 속할 확률 예측
- 시그모이드를 이용하여 확률 예측
- loss func: Binary cross entropy → G.D 이용하여 두 그룹 나누는 직선 $W^{T}x+b$의 W, b (계수) 찾기


## SVM

- Hard Margin SVM : 모든 데이터 올바르게 분류. 오류 허용 X → 과적합
- Soft Margin SVM : 오류 허용, 패널티 부과 → slack var 크사이 추가. 하이퍼파라미터 C로 패널티 조절 (C 커질수록 오류에 민감하게 반응. 마진 좁고, 오버피팅 가능)

### Hard Margin SVM
- 클래스를 구분하는 최적의 초평면을 찾는 것 → SV와 초평면 사이 거리 margin 최대화
- 초평면은 서포트 벡터들로 결정 됨 (초평면에 가장 근접한 데이터 점들)
- loss func: 2Margin 최대화 → $2/|W|$ 최대화 → |W| 최소화 → $\frac{1}{2}|W|^2$  최소화 하는 W, b 찾기
→ Lagrange Multiplier 이용하여 최적화. 

#### Lagrange Multiplier
- Lagrange Multiplier은 Gradient f와 Gradient g가 평행하다는 것 이용
 Why?
 1. Sol은 tangent point (접점)에서 생기기 때문에 항상 평행
 2. ???????????????????

[Primal form]
lagrange func 최소화. W,b에 대한 편미분 = 0 에서 얻은 식들을 바탕으로 식을 정리하면 알파에 대한 식 나옴. (여기서 알파가 L.M) 

[Dual form]
위에서 정리한 식(Lagrange Dual func)을 최대화. (duality gap 줄이기 위해)

#### KKT condition

$$
\nabla f(\mathbf{w^*}) - \sum_{i=1}^{N} \alpha_i \nabla g_i(\mathbf{w^*}) = 0, \quad \text{(stationary)}
$$

$$
\alpha_i g_i(\mathbf{w^*}) = 0, \quad \forall i \in [1, N] \quad \text{(complementary slackness)}
$$

$$
\alpha_i \geq 0, \quad \forall i \in [1, N] \quad \text{(dual feasibility)}
$$

$$
g_i(\mathbf{w^*}) \geq 0, \quad \forall i \in [1, N] \quad \text{(primal feasibility)}
$$


#### PSD
Multivariate func(다변수 함수)의 convex 확인하고 싶어서!!!
	single variable func(일변수 함수)는 두번 미분한 게 양수면 convex

- Hessian Matrix : x에 대해 두번 미분한 값
- Hessian Matrix가 PSD(positive semi definite) 이면 convex하다. 
- PSD : 임의의 실수 x에 대해 $x^{T}Hx >= 0$ 

#### KKT condition 푸는 방법
Step 1. Solve Minimization Problem : Primal form
	w, b, alpha 에 대한 함수 (convex & continuous w,b)

Step 2. Make Lagrange Dual Func : Dual form
	위에서 구한 L(w, b, alpha)에 Optimal W* 대입해서 나온 식. 

Step 3. Find alpha that maximizes Dual form
	QP(Quadratic Programming) 이용해서 Optimal alpha 구하기

→ Non S.V : alpha = 0 / S.V : alpha != 0 → S.V에 대해서만 식
(margin은 SV로만 정의되기 때문에 나머지 애들은 고려 대상 X)

여기서 구한 알파를 바탕으로 Optimal W, b 구하기 → D.B 다 구하게 됨

### Soft Margin SVM
- error cases 허용하고 패널티 부과 
→ Slack Variable
Non SV : Slack Variable = 0
SV on margin : Slack Variable = 0
SV inside and outside the margin(error): Slack Variable > 0

C: 하이퍼 파라미터 → 클수록 DB 넘는 애들 패널티 크게 부여 → Hard Margin SVM과 가까워짐

Lagrange Multiplier 이용해서 풀면 식 다 똑같은데 마지막에 제약 조건 하나만 더 붙음: alpha <= C


### Non-linear SVM
Input space → Feature space (DB 구하기)
nonlinear(mapping) function으로 input space를 고차원으로 보낼 수 있음 

같은 방식으로 Lagrange Multiplier 이용해서 풀면 SVM DB 구할 수 있음
→ 근데 여기서 $\phi(x)^T\phi(x)$ 만 필요함 (내적밖에 안해!! - Scalar value만 필요)

그래서 굳이 Transform 하지 않고도 내적값을 구할 수 있음. 
→ Kernel Trick

#### Kernel Trick
- polynomial kernel
- RBF kernel
- Gaussian kernel
- Sigmoid kernel


### NN
#### 퍼셉트론
- 단순한 이진 분류 모델
- input에 가중치 곱하고 합산하여 활성화 함수를 거쳐 결과 출력
- cost func: MSE → G.D 활용해 W, b 찾기 (가중치와 bias)
- 한계 : 선형적으로 분리 가능한 데이터에서만 작동 →XOR 문제와 같은 

#### Multi layer NN
- 비선형 문제 효과적으로 해결 → 이미지 인식, 자연어 처리 Good 
- 여러개의 은닉층

##### Back Propagation
- feed forward를 통해 input 에서 output 통과시켜 예측값 계산
- loss func를 이용해 오차 계산
- Back Propagation → 체인룰을 통해 각 가중치의 Gradient 계산
- G.D를 통해 가중치 업데이트 

#### SGD 
- 1개의 샘플 사용해 기울기 계산 → 가중치 업데이트
- 빠르지만 불안정

#### GD with Mini batch
- M개 미니 배치. 데이터 무작위 선택. 


#### CNN
- 이미지, 영상데이터 처리하는 신경망 구조
- 합성곱, 풀링, 완전 연결 레이어
- 합성곱: 입력 이미지에 필터 적용하여 특징 맵 추출
- 풀링: 특징 맴의 크기를 줄이고 불필요한 정부 줄여 연산량 감소
- 여러 합성곱, 풀링 레이어 반복 → 고차원적인 특징 추출
- 완전 연결: 추출된 특징 바탕으로 이미지 특정 클래스로 분류

#### RNN
- 순차적인 데이터 처리하는 신경망 구조.
- 이전 단계 출력을 현재 단계 입력으로 활용. 순서 고려하여 패턴 학습
- 입력층 → 순환층(과거 정보 기억) → 출력층
- 모든 시간 단계에서 동일한 가중치 사용
- Hidden state에 이전 단계 정보를 현재 단계에 전달
- BPTT(Back Propagation Through Time) : 현재 시점에 발생한 에러를 과거 시점까지 전달해서 학습
- →시퀀스 길어질수록 Gradient Vanishing 문제 (먼 과거까지 학습 과정에서 곱해지는 G 작아져 기울기 소실)
#### LSTM
- RNN의 장기 의존성 문제 해결
- 기억 셀 추가하여 중요한 정보 오래 기억. 망각 게이트 통해 불필요한 정보 삭제
- 셀 상태와 게이트 구조를 통해 정보 선택적 기억. 삭제
- 망각 게이트와 입력 게이트에서 선택된 정보가 셀 상태(Cell State)를 업데이트
- 기울기 소실 문제 해결. 중요 정보 선택적 기억

#### GRU
- LSTM처럼 장기 의존성 문제 해결. LSTM보다 훈련 속도 빠름
- 입력, 망각 게이트 하나로 통합하여 구조 단순화 : 업데이트, 리셋 게이트


## SVR
- margin : Decision Boundary와 가장 가까운 데이터 사이의 거리 (SV & DB 사이 거리)
- 