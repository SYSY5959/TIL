NeurlPS 2017
- 이 구조 하이라이트 : Attention 3개. 각 Attention 역할!!
- input K,Q 차원이 어떻게 변하는지
# Attention Is All You Need
**“Transformer는 Attention만으로 언어를 모델링할 수 있음을 증명하며,**  
**딥러닝 기반 자연어처리를 순차적 계산에서 병렬적 표현 학습 시대로 전환시킨 패러다임이다.”**

"Transformer demonstrated that attention alone is sufficient for sequence modeling, marking a paradigm shift in NLP from sequential computation to fully parallel representation learning."

## 기존 방법론 한계
- 시퀀스 변환 문제는 주로 RNN/LSTM/GRU 기반 Encoder-Decoder 구조 사용
- → RNN 계열은 입력 시퀀스의 길이에 따라 연산 순차적으로 이루어져서 병렬화 불가능
- → 긴문장에서 장기 의존성 학습 어려움

> “This inherently sequential nature precludes parallelization within training examples…”
> → 이러한 순차적 구조 때문에, 병렬 학습 불가능

→ 순차 계산을 제거하면서도 문맥 정보를 효과적으로 학습할 수 있는 새로운 구조 제안 : Transformer

## Transformer

> “Transformer replaces recurrence with _self-attention_ — attention is all you need.”

- Encoder–Decoder 구조
- RNN을 완전히 제거하고 **Self-Attention + Feed Forward layer**로 구성
- 각 sub-layer에는 **Residual connection + Layer Normalization** 적용

> “We propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.”
→ “본 논문은 순환(recurrence)을 제거하고 오직 attention 메커니즘만으로 입력과 출력 간 전역 의존성을 학습하는 Transformer를 제안한다.”

`입력 문장 → [ 인코더 6층 ] → 중간 표현 → [ 디코더 6층 ] → 출력 문장`

- 인코더는 **“전체 문장을 한꺼번에 보고”** 문맥을 압축하고,
- 디코더는 **“그 압축된 표현을 참고하면서 한 단어씩 예측”** 하는 구조

![](<./Images/7. Transformer_Figure.png>)
### Encoder
- Encoder: 입력 문장 이해. self-attention + feed-forward 네트워크로 구성된 층 6개

$$x_{i}^{embed}​=E^{T}⋅onehot(x_i​)$$
- 임베딩 행렬 : $E∈R^{V×d_{model}}​$ 로 각 단어를 벡터로 바꾸기


```mathematica
입력 →
[ Multi-Head Self-Attention ] → 잔차 연결 + LayerNorm
→ [ Feed Forward Network (FFN) ] → 잔차 연결 + LayerNorm
→ 출력
→ ... (6번 반복) ... 
```

- **Self-Attention**: 문장 내 단어들끼리 서로를 바라봄 (양방향)
- **FFN**: 각 단어를 독립적으로 처리하는 작은 신경망
- **LayerNorm & 잔차 연결**: 학습 안정화 + 정보 보존

> “Encoder: … first is a multi-head self-attention mechanism, and the second is a position-wise feed-forward network… residual… layer normalization… N=6.”  
→ “인코더: 첫 서브레이어는 멀티헤드 self-attention, 두 번째는 위치별 FFN. 잔차+정규화. N=6층.”


### Decoder
- Decoder: 이해한 정보 바탕으로 새로운 문장 생성. masked self-attention, encoder-decoder attention 등을 포함한 층 6개

```mathematica
입력 →
[ Masked Multi-Head Self-Attention ]  → (앞 단어만 바라보기)
→ 잔차 연결 + LayerNorm
→ [ Encoder-Decoder Attention ]       → (입력 문장과 연결)
→ 잔차 연결 + LayerNorm
→ [ Feed Forward Network ]            → (단어 처리)
→ 잔차 연결 + LayerNorm
→ 출력
```

- **Masked Attention**: 출력 문장을 왼쪽 단어까지만 보고 다음 단어 생성 (auto-regressive 구조) (단방향)
- **Encoder-Decoder Attention**: 인코더가 이해한 문맥을 참고해서 문장을 생성함

```scss
Decoder Output (Y_t)  -->  Linear (W^T)  -->  Softmax  -->  P(next token)
          │                          │
          └──── shared with ─────────┘  (input/output embeddings)
```

$$P(y_t​∣y_{<t}​,X)=softmax(Y_{t}​W_{O}^{T}​)$$
- $Y_t$​: 디코더 마지막 층의 t번째 토큰 출력 벡터 (512차원, base model 기준)
- $W_O \in \mathbb{R}^{V \times d_{model}}$: **출력 projection 행렬** → 크기 V로 맞춰주기 위해
- Softmax: 어휘 전체에 대한 확률 분포 (크기 V)

“Transformer는 입력 임베딩과 softmax 직전의 projection 가중치를 공유함으로써,  
**단어 표현 공간의 일관성을 유지하고, 파라미터 수를 줄이며, 학습 효율을 높인다.**”
"By sharing the embedding matrix with the pre-softmax projection, the Transformer maintains a consistent word representation space, reduces the number of parameters, and improves training efficiency."

[파라미터 공유]
> “We share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.”
> → 임베딩 행렬과 출력 projection 행렬 가중치를 공유한다.


[Masked Attention]
> “We modify the self-attention in the decoder… masking… ensures predictions at position i depend only on positions < i.”  
> → “디코더 self-attention은 마스킹을 통해 i 위치의 예측이 오직 이전 위치들에만 의존하게 한다.” 

> “Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.”
> → “기존 시퀀스 변환 모델과 유사하게, 입력·출력 토큰을 $d_{model}$ 차원의 벡터로 변환하기 위한 임베딩을 학습한다. 또한, 디코더의 출력을 학습된 선형 변환과 softmax 함수를 거쳐 다음 토큰 확률을 예측한다."


### 학습

기계번역을 예로 들면,
- **입력(X):** 영어 문장
- **출력(Y):** 독일어 문장
Transformer는 **입력–출력 쌍 (X, Y)** 를 모두 알고 있는 **지도학습(supervised)** 형태로 학습
→ "Encoder가 입력을 인코딩하고, Decoder가 정답 Y를 맞추도록 학습"

|단계|내용|
|---|---|
|(1)|입력 X를 Encoder에 통과시켜 hidden representation Z 계산|
|(2)|Z를 Decoder로 전달, Decoder는 이전 정답 토큰 (y_{<t}^{*})을 입력받아 다음 토큰 예측|
|(3)|Softmax 출력과 실제 정답 (y_t^{*}) 사이에서 cross-entropy loss 계산|
|(4)|이 loss를 기준으로 **Decoder → Encoder 순으로 gradient가 역전파**됨|
|(5)|Encoder와 Decoder의 모든 가중치(Attention, FFN 등)가 동시에 업데이트|
→ **학습 시 Encoder와 Decoder는 동시 업데이트되지만,**  loss는 항상 **Decoder의 출력 예측 성능**에 기반해 계산됨

## Transformer 구성 요소

### Positional Encoding
순서 정보가 없기 때문에 **sinusoidal positional encoding**을 추가
$$PE_{(pos,2i)}​=sin(pos/10000^{2i/d_{model}}​)$$$$PE_{(pos,2i+1)​}=cos(pos/10000^{2i/d_{model}}​)$$
→ 모델이 상대적 위치를 학습할 수 있게 함

> “We add ‘positional encodings’… we use sine and cosine functions… We chose the sinusoidal version because it may allow the model to extrapolate to longer sequences.”
> → sin/cos 위치인코딩을 더한다… 더 긴 시퀀스로의 외삽에 유리할 수 있어 이를 채택했다.


### Feed Forward Layer
$$FFN(x)=max(0,xW_1​+b_1​)W_2​+b_2​$$
→ 각 토큰 위치에 독립적으로 적용되는 비선형 변환

### Attention Mechanism
- Scaled Dot-Product Attention
$$Attention(Q,K,V)=softmax(\frac{(QK^{T})}{\sqrt{d_{k}}}​)×V$$
- Multi-Head Attention
$$MultiHead(Q,K,V)=Concat(head_{1}​,...,head_{h}​)×W^o$$
→ Query, Key, Value를 여러 개(heads)로 분리하여 병렬적으로 서로 다른 부분집합의 정보 학습
→ 다양한 의미에서 관계를 동시에 포착 가능

> “Multi-head attention allows the model to jointly attend to information from different representation subspaces.”  
> “멀티헤드는 서로 다른 표현 부분공간 정보를 동시에 본다.”

![](<./Images/7. Transformer_Figure_1.png>)

#### Encoder Self-Attention


#### Encoder–Decoder Attention (Cross-Attention)


#### Decoder Self-Attention (Masked Self-Attention)

## 결론
Transformer는 최초의 **순수 Attention 기반 Seq2Seq 모델**로서,
- **병렬화 가능성**,
- **장기 의존성 학습**,
- **성능 향상**을 모두 달성

[오직 Attention]
> “We propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies.”  
→ “우리는 순환 구조를 버리고, 오직 attention 메커니즘만으로 전역적 의존성을 학습하는 모델을 제안한다.”

[모델 병렬화]
> “The Transformer allows for significantly more parallelization and can reach a new state of the art after being trained for as little as twelve hours on eight GPUs.”
> → 병렬처리 가능하다..

[Attention의 해석 가능성과 확장성]
> “Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.”
> → Transformer의 각 Attention head들은 서로 다른 역할을 학습하며, 일부 head들은 문장의 문법적 구조나 의미적 관계를 포착하는 방식으로 작동한다.


## 한계점






%%
### Encoder


head 가 갖는 의미가 뭘까????
→ 사물하나를 바라봤을 때, multi head attention : 집중하고 있는 포인트가 각자 다름. 다수의 attention들을 가져가자. head 늘릴수록 다양한 관점을 갖게 됨. 단어에 집중하거나, 문법에 집중하거나... 여러 측면에서 보자.. 노이즈 깨짐. 결과를 보고 해석함. 이런 이유가 아닐까??  
→ 다양한 측면이다. 

[교수님]
- 차원 concat → linear projection : 차원을 다시 맞춰주기 위해서. 출력과 인풋의 차원이 같아야하는데, multi-head 하고 늘려놨다가 다시 줄여놔야함. 그래야 다음 인코더 블럭에 들어갈 준비를 하게 됨

- transformer에서 가장 중요하다고 생각하는 매커니즘?
→ 멀티 해드 어텐션은 계속 있음. self attention
어텐션 3개 있다는 것. 인코더에서 갖는 어텐션, 디코더 마스크드 **인코더 디코더 연결하는 어텐션**이게 키포인트!!!! → 트렌스포머의 핵심 포인트 아키텍쳐. 이렇게 구성한게 성능 향상된 원인.

%%