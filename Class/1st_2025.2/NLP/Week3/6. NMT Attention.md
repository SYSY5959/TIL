EMNLP 2015
# Effective Approaches to Attention-based Neural Machine Translation

## 기존 방법론 한계
[seq2seq]
- 모든 source 정보를 단일 벡터 s에 압축 (정보 손실 심각)
- 긴 문장 번역 시 문맥 유지 불안정
- source-target 간 단어 alignment를 명시적으로 모델링 X
	- “번역 중에 어떤 source 단어가 현재 target 단어 생성에 영향을 주었는지 추적할 수 없다”는 뜻

[Bahdanau et al.(2015)의 attention]
- 디코더가 각 타임스텝마다 encoder hidden states 전체를 참고
- soft alignment를 학습 가능하게 함
- 하지만 **복잡한 구조(양방향 GRU + maxout layer)**,  
    그리고 **attention 함수 형태가 단일(concat)** 로 고정되어 일반화가 어려움

> “Conventional NMT compresses the entire source sentence into a single fixed-length vector, which leads to severe information loss for long sentences and prevents the model from explicitly learning source–target word alignments.  
> → 기존 NMT는 전체 문장을 하나의 벡터로 압축하여 긴 문장에서 정보 손실이 심하고, 소스-타겟 단어 간 정렬 관계를 명시적으로 학습하지 못함
> Although Bahdanau’s attention mechanism addresses this limitation through soft alignment, its architecture is complex and narrowly designed, resulting in constraints on efficiency and generalization.”
> → Bahdanau의 attention은 이를 soft alignment로 해결했지만, 구조가 복잡하고 설계가 제한적이라 효율성과 일반화 측면에서 한계가 있었다.

→ 더 단순하고 효과적인 Attention 구조 제안

> “Our global attention approach is similar in spirit to Bahdanau et al. (2015), but we simplify and generalize their model.”


## NMT Attention
stacked LSTM-based Encoder–Decoder 구조

- **Encoder:** 여러 층의 LSTM으로 source 단어 시퀀스를 인코딩 → 각 hidden state $\bar{h}_s$​ 생성
- **Decoder:** target 단어를 생성하면서, 각 시점 t에서 **attention**으로 source 문맥을 요약한 벡터 $c_t$ 를 계산
- **Output:** $h_t$​ (디코더 state)와 $c_t$​를 결합해 단어 확률 예측

> “Luong et al. (2015) proposed three simple yet effective modifications to attention-based NMT:  
> a **Global Attention** that focuses on all source words, a **Local Attention** that restricts focus to a predicted subset, and an **Input-feeding** mechanism that incorporates past alignment information.  
> Together, these innovations achieved state-of-the-art translation quality while simplifying and accelerating the original Bahdanau model.”


### Global Attention
- 모든 source 단어 (hidden states)를 대상으로 attention 계산
- 구조 더 단순화 & 다양한 scoring 함수 (dot / general / concat) 비교

```mathematica
Encoder (Bidirectional LSTM or LSTM stack)
h̄1   h̄2   h̄3   ...   h̄S
 ↓     ↓     ↓          ↓
  \   / \   / \        /
   \ /   \ /   \      /
    Alignment scores
         ↓
   Softmax over all source positions → a_t(s)
         ↓
Weighted sum of all h̄s → Context vector c_t
         ↓
Concatenate [c_t ; h_t] → attentional hidden state h̃_t
         ↓
Softmax(W_s h̃_t) → predicted word y_t
```


- alignment vector
$$
a_t(s) = \frac{\exp(\text{score}(h_t, \bar{h}_s))}{\sum_{s'} \exp(\text{score}(h_t, \bar{h}_{s'}))}
$$
- context vector
$$
c_t = \sum_s a_t(s) \bar{h}_s
$$
- hidden state update
$$
\tilde{h}_t = \tanh(W_c [c_t; h_t])
$$
- 최종 예측
$$
p(y_t \mid y_{<t}, x) = \text{softmax}(W_s \tilde{h}_t)
$$

- score 함수

| Type    | Formula                                | 특징               |
| ------- | -------------------------------------- | ---------------- |
| dot     | $h_t^\top \bar{h}_s$                   | 간단하고 빠름          |
| general | $h_t^\top W_a \bar{h}_s$               | 가중치 행렬 포함        |
| concat  | $v_a^\top \tanh(W_a [h_t; \bar{h}_s])$ | Bahdanau 방식 (복잡) |


![](<./Images/6. NMT Attention_Figure.png>)

### Local Attention
- 모든 source 단어를 보지 않고, target 단어마다 **예측된 중심 위치 $p_t$** 근처의 단어들만 attention
- → 계산량 대폭 감소. 긴 문장 번역에서 성능 안정적

```css
Encoder outputs: h̄1, h̄2, ..., h̄S
                           ↑
                     predicted position p_t
                     ← window (p_t - D, p_t + D) →
                           ↓
                 Alignment scores only within window
                           ↓
            Softmax over local window → a_t(s)
                           ↓
Weighted sum → Context vector c_t
                           ↓
Concatenate [c_t ; h_t] → attentional hidden state h̃_t
                           ↓
Softmax(W_s h̃_t) → predicted y_t
```

- 예측된 중심 위치 $p_t$
$$
p_t = S \cdot \text{sigmoid}(v_p^\top \tanh(W_p h_t))
$$
- 주변 윈도우 $[p_t - D, p_t + D]$ 설정
- alignment vector
	- 중심 주변에 가우시안 분포를 적용 → 가까운 단어일수록 높은 가중치
$$
a_t(s) = \text{align}(h_t, \bar{h}_s) \cdot \exp\left( -\frac{(s - p_t)^2}{2\sigma^2} \right)
$$
- context vector
$$
c_t = \sum_{s = p_t - D}^{p_t + D} a_t(s) \bar{h}_s
$$

![](<./Images/6. NMT Attention_Figure_1.png>)


### Input -feeding Approach
- 기존 attention은 시점마다 독립적으로 작동해서, '과거에 어떤 부분을 이미 번역했는가?'를 고려 못 함.
→ 이전에 어디를 봤는지(coverage) 정보 X
→ 이를 해결하기 위해: 이전 단계의 attention hidden state $\tilde{h}_{t-1}$ 을 다음 단계 LSTM 입력에 추가

$$
x_t' = [y_{t-1}; \tilde{h}_{t-1}]
$$
→" 이미 번역된 부분"을 기억하고, "새로운 attention을 어디에 둘지" 더 안정적으로 결정

```css
At time t-1:
   ↓
Attentional vector h̃_{t-1} (summarizes where model attended)
   ↓
Feed this vector as part of next input:
   x_t' = [y_{t-1} ; h̃_{t-1}]
   ↓
Pass through LSTM decoder → new hidden state h_t
   ↓
Compute new attention (global/local)
   ↓
Predict next word y_t
```

![](<./Images/6. NMT Attention_Figure_2.png>)

%% y 생성할 때, 누적되면서 정보 쌓임. A,B,C,D 다 들어감%%

## Ablation study

| Ablation 구성 요소                              | 실험 설정 / 설명                                 | 성능 변화 (BLEU ↑) | 참조 위치 (논문 내 표·그림) | 비고 / 해석                                   |
| ------------------------------------------- | ------------------------------------------ | -------------- | ----------------- | ----------------------------------------- |
| **Base model (seq2seq)**                    | 기본 LSTM 4-layer, no attention, no dropout  | 11.3 BLEU      | Table 1 (p.13)    | Baseline (non-attentional NMT)            |
| + **Source reversing**                      | 입력 문장을 역순으로 넣어 학습                          | +1.3 (→ 12.6)  | Table 1           | Sutskever trick 적용, gradient alignment 개선 |
| + **Dropout**                               | LSTM layer에 dropout 0.2 적용                 | +1.4 (→ 14.0)  | Table 1           | overfitting 방지, 일반화 향상                    |
| + **Global Attention (location-based)**     | attention 추가, 모든 source 단어에 soft alignment | +2.8 (→ 16.8)  | Table 1 & Fig. 2  | 최초 attention 적용, 의미적 alignment 획득         |
| + **Input-feeding**                         | 이전 step의 attention vector를 다음 입력에 피드백      | +1.3 (→ 18.1)  | Table 1 & Fig. 4  | coverage-like 효과, alignment 일관성 향상        |
| + **Local Attention (predictive, general)** | attention을 window 내에서만 수행, 중심 위치 pt 예측     | +0.9 (→ 19.0)  | Table 1 & Fig. 3  | 효율 향상, sharper alignment                  |
| + **UNK replacement**                       | unknown token을 alignment 기반으로 원문 단어로 치환    | +1.9 (→ 20.9)  | Table 1           | attention의 practical alignment 품질 검증      |
| **Ensemble (8 models)**                     | 서로 다른 attention 설정 모델을 앙상블                 | +2.1 (→ 23.0)  | Table 1           | 최종 BLEU +5.0 향상 달성 (SOTA)                 |
## 결론
“Attention이 NMT의 부속 기능이 아니라, 핵심 구조가 될 수 있음을 증명한 논문”
1. Attention을 체계화하고 실증적으로 검증한 첫 연구
2. Attention을 alignment function으로 명확히 정의

> “In this paper, we propose two simple and effective attentional mechanisms for neural machine translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time. … Our local attention yields large gains of up to 5.0 BLEU over non-attentional models … For the English to German translation direction, our ensemble model has established new state-of-the-art results.”
> → 본 논문에서는 NMT를 위한 두 가지 간단하면서도 효과적인 어텐션 메커니즘을 제안하였다. 하나는 모든 소스 위치를 참조하는 Global Attention이며, 다른 하나는 일부 위치에만 집중하는 Local Attention이다.  Local Attention은 기존 비어텐션 모델 대비 BLEU 점수에서 최대 5.0의 향상을 보였으며, 영어→독일어 번역에서 새로운 SOTA 결과를 달성하였다.”


## 한계점
1. RNN 기반 구조 한계
	-  LSTM 기반의 seq2seq 구조 위에 Attention 얹은 형태
	- attention이 있어도 문맥 정보 전달의 기본은 LSTM의 hidden state에 의존
	- → LSTM은 본질적으로 순차적 연산(sequential computation) 이라, 병렬화 불가능 + 긴 문장에서는 여전히 정보 소실 발생
	→ *Transformer* : **RNN 제거**, 모든 연산을 attention으로 대체 → 완전 병렬화 + 긴 문장에서도 안정적

2. Attention이 여전히 단일 모드(single-head)
	- 매 시점마다 단 하나의 가중합(context vector) → 모델이 문맥의 여러 측면(구문적, 의미적, 위치적)을 동시에 볼 수 없음
	- → 문맥의 다양한 관계 포착 어려움. 복잡한 문장 구조에 약함
	→ *Transformer*: **Multi-head Attention** 도입 → 여러 독립적 attention head로 다양한 관계 병렬 학습
3. Coverage(피드백)는 단순 Input-feeding 수준
	- “input-feeding”은 단순히 이전 attention 결과를 입력으로 다시 넣는 구조
	- 이건 coverage(얼마나 source를 번역했는가)를 명시적으로 추적하지 않음
	- → 중복 번역, 누락 발생 가능
	→ *Transformer*: **Residual connection + layer stacking**을 통해 attention 정보의 누적 효과를 안정적으로 통합
4. Local Attention의 제한된 수용 범위
	- Local attention는 window 밖의 단어를 전혀 보지 못함
	- → 문장 내 long-range dependency 포착 어려움. reorder가 심한 언어쌍에서는 성능 한계
	→ *Transformer*: 완전 연결된 **self-attention**으로 모든 위치 간 의존성을 직접 학습
5. Context representation 단층 구조
	- 단일 layer의 context representation 만 사용
	- → attention 결과를 한번만 결합하고, 심층 표현 통합 X
	→ *Transformer* : 여러 layer에서 반복적으로 attention 수행 (**multi-layer stacked self-attention**)









