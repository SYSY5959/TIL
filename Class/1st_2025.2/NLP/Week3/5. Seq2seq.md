NeurlPS 2014
# Sequence to Sequence Learning with Neural Networks

## ê¸°ì¡´ ë°©ë²•ë¡  í•œê³„
1. DNNì˜ êµ¬ì¡°ì  í•œê³„
- ê¸°ì¡´ì˜ DNNì€ ê³ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ, ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê¸¸ì´ê°€ ê³ ì •ëœ ë²¡í„°ì—¬ì•¼ í•¨
- â†’ sequence-to-sequence ë¬¸ì œ(ex. ê¸°ê³„ ë²ˆì—­, ìŒì„± ì¸ì‹)ì— ì ìš©í•˜ê¸° ì–´ë ¤ì› ìŒ

> â€œDespite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.â€


2. RNNì˜ êµ¬ì¡°ì  í•œê³„
- ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ê³ , ë³µì¡í•œ non-monotonic ê´€ê³„ë¥¼ ê°€ì§„ ë¬¸ì œì—ëŠ” ì ìš©í•˜ê¸° ì–´ë ¤ì›€

> â€œThe RNN can easily map sequences to sequences whenever the alignment between the inputs and the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and output sequences have different lengths with complicated and non-monotonic relationships.â€

3. ì‹ ê²½ë§ ê¸°ë°˜ ë²ˆì—­ ì‹œë„ í•œê³„
- ë¬¸ì¥ì„ ë²¡í„°ë¡œ ì••ì¶• â†’ ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ ì†ì‹¤, ë¬¸ì¥ ìˆœì„œ ì •ë³´ ìœ ì‹œ ì‹¤íŒ¨
- end-to-end neural translation ì—¬ì „íˆ ë¯¸í•´ê²° ìƒíƒœ

> â€œOur approach is closely related to Kalchbrenner and Blunsom [18] who were the first to map the entire input sentence to a vector, and is related to Cho et al. [5] although **the latter was used only for rescoring hypotheses produced by a phrase-based system.**â€
> â†’ ê¸°ì¡´ì˜ phrase-based ì‹œìŠ¤í…œì´ ìƒì„±í•œ í›„ë³´ ë¬¸ì¥ì„ ì¬í‰ê°€(rescoring) í•˜ëŠ” ë°ë§Œ ì‚¬ìš©ë˜ì—ˆë‹¤. 


## Seq2seq

ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ **LSTM encoder**ë¡œ ì¸ì½”ë”©í•´ **ê³ ì • ê¸¸ì´ ë²¡í„°**ë¡œ ìš”ì•½í•˜ê³ , ì´ë¥¼ **LSTM decoder**ê°€ ë°›ì•„ **ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±**í•œë‹¤.

> â€œUse one LSTM to read the input sequence â€¦ and then another LSTM to extract the output sequence from that vector.â€
> â€œWe used two different LSTMs: one for the input sequence and another for the output sequence...â€


$$
p(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T)
= \prod_{t=1}^{T'} p(y_t \mid v, y_1, \ldots, y_{t-1})
$$
- **Encoder LSTM**: ì…ë ¥ ë¬¸ì¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ì½ì–´ë“¤ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ **ê³ ì • ì°¨ì› ë²¡í„°** vë¡œ ì••ì¶•
- **Decoder LSTM**: í•´ë‹¹ ë²¡í„°ë¥¼ ì´ˆê¸° ìƒíƒœë¡œ ë°›ì•„ì„œ **ì¶œë ¥ ë¬¸ì¥**ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„± (ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ì˜ˆì¸¡)

> â€œThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector.â€
> â†’ **ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í•œ ì‹œì ì”© ì½ì–´ ê³ ì •ëœ ì°¨ì›ì˜ ë²¡í„° í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ í•˜ë‚˜ì˜ LSTM**ì„ ì‚¬ìš©í•˜ê³ , **ê·¸ ë²¡í„°ë¡œë¶€í„° ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ë˜ ë‹¤ë¥¸ LSTM**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•µì‹¬ ì•„ì´ë””ì–´


```css
Input:   A, B, C, <EOS>
Encoder: Hidden_3 = Context vector (v)
Decoder: v â†’ W, X, Y, Z, <EOS>
```

### êµ¬ì¡°

![](<./Images/5. Seq2seq_Figure.png>)

#### LSTM : cell state + gating mechanism

- LSTMì€ â€œgradient vanishingâ€ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ **cell state + gating mechanism**ì„ ê°–ê³  ìˆì–´ì„œ,  
- ê¸´ ì‹œí€€ìŠ¤ì—ì„œë„ **ì¥ê¸° ì˜ì¡´ì„±(long-term dependency)** ì„ ë³´ì¡´í•  ìˆ˜ ìˆìŒ

*cell state + gating mechanismì— ëŒ€í•œ ì„¤ëª…ì€ ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ X*
> â€œThe LSTMâ€™s ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application(MT) due to the considerable time lag between the inputs and their corresponding outputs (fig. 1).â€
> â†’ LSTMì€ ì¥ê¸°ì ì¸ ì‹œê°„ ì˜ì¡´ì„±ì„ ê°€ì§„ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì§€ë‹ˆê³  ìˆê¸° ë•Œë¬¸ì—, ì…ë ¥ê³¼ ì¶œë ¥ ê°„ì— ìƒë‹¹í•œ ì‹œê°„ ì§€ì—°(time lag)ì´ ì¡´ì¬í•˜ëŠ” ì´ ë¬¸ì œ(ë²ˆì—­)ì— **ìì—°ìŠ¤ëŸ¬ìš´ ì„ íƒì§€**ê°€ ëœë‹¤.â€

#### Reversing Trick : ìµœì í™” ë‚œì´ë„ í•´ê²°

- ì…ë ¥ ë¬¸ì¥ì„ ë’¤ì§‘ìœ¼ë©´, ë²ˆì—­ ì´ˆë°˜ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì´ sourceì˜ ë§ˆì§€ë§‰ ë‹¨ì–´ë“¤ê³¼ **ê°€ê¹Œìš´ ì‹œì (timestep)** ì— ìœ„ì¹˜í•˜ê²Œ ë¨
- â†’ **Backpropagationì˜ ì‹œê°„ ì§€ì—°(time lag)** ì´ ì¤„ì–´ë“¦
- â†’ **Gradient flowê°€ ë” ì•ˆì •ì ** â†’ í•™ìŠµ ìˆ˜ë ´ì´ ì‰¬ì›Œì§

```scss
original> Input: [A  B  C]

Reversed>
Input:   [C  B  A]
Output:  [Î±  Î²  Î³]
A â†” Î± (ê°€ê¹Œì›€)
```
â†’ ì²« ë²ˆì§¸ ë²ˆì—­ ë‹¨ì–´ Î±ëŠ” Aì™€ ë” ê°€ê¹Œìš´ ì‹œì (timestep)ì— ëŒ€ì‘  
â†’ í•™ìŠµ ì´ˆê¸°ì— alignment í˜•ì„± ì‰¬ì›€, SGDê°€ ë¹ ë¥´ê²Œ ìˆ˜ë ´

> â€œThe Reversing Trick reduces the time lag between the input and output sequences by reversing the source sentence order, allowing SGD to learn the alignment more easily.â€

#### Deep LSTM êµ¬ì¡° : í‘œí˜„ë ¥ ì¦ê°€
- 4-layer LSTM â†’ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ë” ì •êµí•˜ê²Œ ì¸ì½”ë”© ê°€ëŠ¥

> â€œWe found that deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers.â€

[ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ë“¤ì–´ì˜¤ë©´, ê° ì‹œì  tì—ì„œ]
```scss
x_t â†’ [LSTM layer 1] â†’ h_t^(1)
      â†“
    [LSTM layer 2] â†’ h_t^(2)
      â†“
    [LSTM layer 3] â†’ h_t^(3)
      â†“
    [LSTM layer 4] â†’ h_t^(4)
```
- í•˜ë‚˜ì˜ time stepì—ì„œ 4ìŒì˜ hidden/cell state ì¡´ì¬
- hidden state: $h_t^{(1)}, h_t^{(2)}, h_t^{(3)}, h_t^{(4)}$
- cell state: $c_t^{(1)}, c_t^{(2)}, c_t^{(3)}, c_t^{(4)}$

#### í•™ìŠµ ê·œëª¨ & ë³‘ë ¬í™”
- ë‹¹ì‹œë¡œì„  ë§¤ìš° ëŒ€ê·œëª¨(12M ë¬¸ì¥, 384M íŒŒë¼ë¯¸í„°)ë¥¼ 8 GPU ë³‘ë ¬ í•™ìŠµìœ¼ë¡œ ì²˜ë¦¬
â†’ ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ **Neural Translationì´ SMTë¥¼ ëŠ¥ê°€**í•  ìˆ˜ ìˆìŒì„ ìµœì´ˆë¡œ ì…ì¦í•¨

> â€œEach layer of the LSTM was executed on a different GPU â€¦ achieved a speed of 6,300 words/sec.â€

### Ablation study

| Ablation ìš”ì¸       | ì‹¤í—˜ ì„¤ì •        | ì„±ëŠ¥ ë³€í™” (BLEU)      | ë…¼ë¬¸ ë‚´ ìœ„ì¹˜  | í•µì‹¬ í•´ì„            |
| ----------------- | ------------ | ----------------- | -------- | ---------------- |
| ì…ë ¥ ìˆœì„œ (Reversing) | ì›ë˜ â†” ì—­ìˆœ      | +4.7              | Sec. 3.3 | ì‹œê°„ ì§€ì—° ê°ì†Œ, í•™ìŠµ ì•ˆì •í™” |
| ëª¨ë¸ ê¹Šì´             | 1ì¸µ â†” 4ì¸µ      | perplexity ì•½ -30% | Sec. 3.4 | í‘œí˜„ë ¥ ì¦ê°€, ë¬¸ë§¥ í•™ìŠµ    |
| Beam size         | 1 â†” 12       | +0.3~0.5          | Table 1  | íƒìƒ‰ ì•ˆì •í™”           |
| Ensemble í¬ê¸°       | 1 â†” 5        | +1.5              | Table 1  | ì˜ˆì¸¡ í‰ê· í™”, BLEU í–¥ìƒ  |
| Gradient clipping | ì‚¬ìš© â†” ë¯¸ì‚¬ìš©     | ì•ˆì •ì„± í–¥ìƒ            | Sec. 3.4 | í­ë°œ ë°©ì§€            |
| Batch ì •ë ¬          | ë¬´ì‘ìœ„ â†” ê¸¸ì´ë³„ ì •ë ¬ | 2Ã— í•™ìŠµì†ë„ â†‘         | Sec. 3.4 | GPU íš¨ìœ¨ í–¥ìƒ        |
## ê²°ë¡ 
1. ìµœì´ˆë¡œ **ì™„ì „í•œ End-to-End Neural Translation ëª¨ë¸** ì‹¤í˜„
â†’ ê¸°ì¡´ì˜ SMTì™€ ë‹¬ë¦¬, feature ì—”ì§€ë‹ˆì–´ë§, phrase table, alignment rule ì—†ì´ ë‹¨ìˆœíˆ ë³‘ë ¬ ë¬¸ì¥ ë°ì´í„°ë¡œë¶€í„° ë²ˆì—­ì„ â€œí•™ìŠµâ€í•œ ì²« ëª¨ë¸
*â€œì´ì œëŠ” ì‚¬ëŒì´ ê·œì¹™ì„ ì§œëŠ” ë²ˆì—­ì´ ì•„ë‹ˆë¼, ë°ì´í„°ë¡œë¶€í„° ë²ˆì—­ ê·œì¹™ì„ â€˜í•™ìŠµâ€™í•˜ëŠ” ëª¨ë¸ì´ ê°€ëŠ¥í•˜ë‹¤.â€*

> â€œWe present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.â€

|ê¸°ì—¬|ì„¤ëª…|
|---|---|
|ğŸ§  **Encoderâ€“Decoder êµ¬ì¡° ì œì•ˆ**|ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ê³ ì • ë²¡í„°ë¡œ ì¸ì½”ë”©, ê·¸ ë²¡í„°ë¡œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” end-to-end framework|
|ğŸ” **LSTMì„ ì´ìš©í•œ long-range dependency ì²˜ë¦¬**|ì…ë ¥ê³¼ ì¶œë ¥ ê°„ ê¸´ ì‹œê°„ ì§€ì—°(time lag) ë¬¸ì œ í•´ê²°|
|ğŸ”„ **Reversing Trick ë„ì…**|ì…ë ¥ ìˆœì„œ ì—­ì „ìœ¼ë¡œ í•™ìŠµ ë‚œì´ë„ ê°ì†Œ, BLEU +4.7 ìƒìŠ¹|
|ğŸ§© **Deep 4-layer LSTM ì‚¬ìš©**|ëª¨ë¸ ê¹Šì´ í™•ì¥ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ì…ì¦|
|ğŸ’¡ **End-to-End NMTì˜ ê°€ëŠ¥ì„± ì…ì¦**|phrase-based SMTë¥¼ ì²˜ìŒìœ¼ë¡œ ëŠ¥ê°€í•œ neural model|
## í•œê³„ì 
1. Fixed-length Context Vector
	- ëª¨ë“  ì…ë ¥ ë¬¸ì¥ì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ìš”ì•½í•´ì•¼í•¨ â†’ ê¸´ ë¬¸ì¥ì¼ìˆ˜ë¡ ì •ë³´ ì†ì‹¤ ë°œìƒ
	â†’ *Transformer ë“±ì¥*

2. OOV ë¬¸ì œ
	- ë‹¨ì–´ ì‚¬ì „ì„ 80kë¡œ ì œí•œí–ˆê¸° ë•Œë¬¸ì—, ê·¸ ë°–ì˜ ë‹¨ì–´ëŠ” ëª¨ë‘ â€œUNKâ€ë¡œ ì²˜ë¦¬ë¨
	- ì‹ ì¡°ì–´ë‚˜ í¬ê·€ ë‹¨ì–´ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥ â†’ *subword ëª¨ë¸ (BPE, 2015) ë“±ì¥*
> â€œThe LSTMâ€™s BLEU score was penalized on out-of-vocabulary words.â€

3. ë³‘ë ¬í™” í•œê³„, í•™ìŠµ ì†ë„
	- LSTMì€ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•´ì„œ ë³‘ë ¬ ì—°ì‚°ì´ ì–´ë ¤ì›€
	- â†’ *Transformer ë“±ì¥*


[ë…¼ë¬¸ í•œì¤„ ìš”ì•½]
> â€œSutskever et al. (2014) introduced the first end-to-end sequence-to-sequence model with LSTMs, marking the beginning of neural machine translation. Although limited by fixed-length encoding and sequential computation, it laid the foundation for attention mechanisms and the Transformer architecture that redefined modern NLP.â€




%%
## ë°œí‘œ

DNN : ì…ë ¥ê³¼ ì¶œë ¥ ê³ ì •

ì‹¤í—˜ - í¬ê·€í•œ ë‹¨ì–´ì—ì„œëŠ” ì„±ëŠ¥ ë¹„ìŠ·í•´ì§ 
ì§§ì€ ë¬¸ì¥, ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ â†’ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ì„±ëŠ¥ ë›°ì–´ë‚¨

ì•™ìƒë¸” ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ë” ì¢‹ì•˜ëŠ”ë° ì–´ë–¤ ì‹ìœ¼ë¡œ ì§„í–‰ëœê±´ì§€?

MT ì˜ ê¸°ì—¬í•œ ë…¼ë¬¸ì´ë‹¤

LSTM ë„ ë‹¨ì ì´ ìˆë‹¤. â†’ Attention

%%