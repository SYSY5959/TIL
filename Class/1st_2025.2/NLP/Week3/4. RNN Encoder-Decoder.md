EMNLP 2014
# Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation

**“RNN Encoder–Decoder는 입력 시퀀스를 하나의 의미 벡터로 압축하고, 그 벡터로부터 새로운 시퀀스를 생성하는 첫 번째 신경망 구조이며, 이후 모든 Seq2Seq·NMT 모델의 토대가 되었다.”**

## 기존 방법론 한계
- **기존 SMT**는 통계적으로 자주 등장하는 구문만을 학습했기 때문에,  
- **희귀 구문, 긴 문장, 순서가 중요한 표현**을 처리하지 못함
→ **“단어 순서와 문맥 정보를 보존하면서, 가변 길이 시퀀스를 다루는 신경망”** 필요
→ RNN Encoder–Decoder 구조 & GRU 도입

1. 입력/출력 길이 고정 → 가변 길이 시퀀스 처리 불가
> Schwenk (2012) proposed a similar approach of scoring phrase pairs. Instead of the RNN-based neural network, he used a feedforward neural network that has fixed-size inputs (7 words in his case, with zero-padding for shorter phrases) and fixed-size outputs (7 words in the target language)
> → 입력·출력 길이가 고정된 피드포워드 네트워크를 사용
> They reported an impressive improvement, but their approach still requires the maximum length of the input phrase (or context words) to be fixed a priori.”

2. 단어 순서 무시 (bag-of-words)
> The RNN Encoder–Decoder naturally distinguishes between sequences that have the same words but in a different order, whereas the aforementioned approaches effectively ignore order information.”
> → RNN 인코더-디코더와 다르게, 기존 모델은 단어의 순서를 고려하지 X


3. 빈도 기반 통계 모델 → 희귀 구문 처리 어려움
> One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase pairs in the original corpus.
> → 기존의 phrase table이 빈도 기반 통계 정보를 반영하고 있어서, 
> With a fixed capacity of the RNN Encoder–Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i.e., distinguishing between plausible and implausible translations, or learning the “manifold” (region of probability concentration) of plausible translations.”
> → RNN 인코더-디코더는 제한된 용량을 언어의 규칙성, 의미적으로 plausible 번역과 부자연스러운 번역을 구분하도록 함

4. LSTM의 복잡도 높음 → GRU
> “In addition to a novel model architecture, we also propose a new type of hidden unit (f in Eq. (1)) that has been motivated by the LSTM unit but is much simpler to compute and implement.
> → LSTM에서 영감을 받았지만, 계산과 구현이 훨씬 단순한 hidden unit 제안
> The LSTM unit … has a memory cell and four gating units that adaptively control the information flow inside the unit, compared to only two gating units in the proposed hidden unit.
> → LSTM은 4개 셀 게이트 사용하지만, 제안된 유닛은 2개의 게이트만 사용

## RNN Encoder-Decoder

> “We propose a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence.”
> → 가변 길이의 시퀀스를 고정 길이 벡터로 인코딩하고, 그 벡터를 다시 가변 길이의 시퀀스로 디코딩하도록 학습하는 새로운 신경망 구조 제안

- **Encoder RNN:** 입력 시퀀스를 하나의 벡터 $c$로 요약
- **Decoder RNN:** 요약 벡터 $c$를 이용해 출력 시퀀스를 생성

![](<./Images/4. RNN Encoder-Decoder_Figure.png>)

```css
x1 → x2 → ... → xT   → [Encoder] →  c  → [Decoder] → y1 → y2 → ... → yT'
```

#### Encoder
**input → Encoder → 벡터 c (요약)**
- variable-length sequence → fixed-length vector representation
- 입력 시퀀스를 순차적으로 읽으며, hidden state 업데이트
- → 마지막 hidden state $h_{T}$ 를 전체 입력 시퀀스의 요약 c로 사용
$$h_{⟨t⟩} = f(h_{⟨t−1⟩}, x_t)$$
$$c = h_{T}$$
#### Decoder
**벡터 c → Decoder → output**
- fixed-length vector representation → variable-length sequence
- 요약 벡터 c를 조건으로, 순차적으로 출력 생성
$$
h_{⟨t⟩} = f(h_{⟨t-1⟩}, y_{t-1}, c)
$$
$$
p(y \mid x) = \prod_{t=1}^{T'} p(y_t \mid y_{<t}, c)
$$
#### 파라미터 학습
- RNN Encoder–Decoder 공동으로 학습 → 조건부 log-likelihood 최대화 되도록!
$$
\max_{\theta} \frac{1}{N} \sum_{n=1}^{N} \log p_{\theta}(y_n | x_n)
$$
$$y_t​=Decoder(c,y_{t−1}​)=Decoder(Encoder(x),y_{t−1}​)$$
### GRU

![](<./Images/4. RNN Encoder-Decoder_Figure_1.png>)

#### Reset gate
**이전 hidden state $h^{⟨t-1⟩}$에 얼마나 의존할지 결정**
→ 앞으로 필요 없을 정보를 hidden state에서 효과적으로 제거
$$
r_j = \sigma\bigl([W_r x]_j + [U_r h_{⟨t-1⟩}]_j\bigr)
$$
- **r 값이 0에 가까우면**: 이전 state 정보 거의 무시 → **새로운 입력에만 의존**
- r 값이 1에 가까우면: 이전 state 정보 그대로 유지

> The reset gate r decides whether the previous hidden state is ignored.

##### Candidate hidden state $\tilde{h}$
reset gate가 조절한 이전 hidden state와 현재 입력 $x$를 결합해 **새로운 후보 hidden state** 생성

#### update gate
**기존 hidden state $h^{⟨t-1⟩}$와 새로운 후보 $~\tilde{h}$를 얼마나 섞을지 결정
$$
z_j = \sigma\bigl([W_z x]_j + [U_z h_{⟨t-1⟩}]_j\bigr)
$$
**z 값이 0에 가까우면: 새로운 후보 state로 대체**
- z 값이 1에 가까우면: 이전 state를 더 유지


> “The update gate controls how much information from the previous hidden state will carry over to the current hidden state. 
> This acts similarly to the memory cell in the LSTM network and helps the RNN to remember long-term information.”
> → update gate는 과거 상태를 얼마나 유지할지 결정. LSTM의 memory cell과 유사하게 작동

> The update gate z selects whether the hidden state is to be updated with a new hidden state $\tilde{h}$ 

##### hidden state
$$
h_j^{⟨t⟩} = z_j \cdot h_j^{⟨t-1⟩} + (1 - z_j) \cdot \tilde{h}_j^{⟨t⟩}
$$
### SMT
- RNN Encoder–Decoder는 **완전한 번역 시스템**으로 쓰이지 않고,  
- 기존 **phrase-based SMT의 phrase table에 새로운 확률 feature로 추가**됨 → *통계 기반 모델에 semantic feature 추가*

> “We propose to train the RNN Encoder–Decoder on a table of phrase pairs and use its scores as additional features in the log-linear model when tuning the SMT decoder.”

- 기존 SMT: phrase pair의 통계적 확률만 사용
- RNN Encoder–Decoder: phrase pair의 **언어적 적합성(semantic plausibility)** 을 추가 점수로 반영

### Ablation study

| 실험 조건                            | 제거/추가 요소               | 관찰된 효과            | 의미                           |
| -------------------------------- | ---------------------- | ----------------- | ---------------------------- |
| Baseline                         | RNN 없음                 | —                 | SMT 통계 기반 성능 기준              |
| +RNN                             | RNN Encoder–Decoder 추가 | BLEU +0.57        | 의미적 representation 효과        |
| +RNN + CSLM                      | Neural LM 추가           | BLEU +1.34        | 서로 보완적 feature               |
| +RNN + CSLM + WP                 | Word penalty 추가        | dev만 소폭 ↑, test ↓ | UNK 보정 효과 미미                 |
| Translation model ↔ RNN score 비교 | 통계적 vs 의미적 scoring     | 의미적 regularity 포착 | RNN이 linguistic structure 학습 |

## 결론
- 최초로 **"가변 길이의 입력을 신경망으로 인코딩 → 다른 길이의 출력으로 디코딩"** 하는 구조 제안
>“In this paper, we proposed a new neural network architecture, called an RNN Encoder–Decoder, that is able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length.”

- 의미적으로 더 자연스러운 번역 생성 → 모델이 언어적 규칙성(linguistic regularity)을 내재적으로 학습했음
> “In most cases, the choices of the target phrases by the RNN Encoder–Decoder are closer to actual or literal translations.  
> We can observe that the RNN Encoder–Decoder prefers shorter phrases in general.”
> → RNN Encoder–Decoder가 선택한 번역 구문이 실제 번역과 더 가깝다. 또한 이 모델은 일반적으로 더 간결하고 자연스러운 구문을 선호한다

- 모델이 문장구조 + 의미적 유사성 모두 내재화
> “From the visualization, it is clear that the RNN Encoder–Decoder captures both semantic and syntactic structures of the phrases.”

- SMT에서 성능 향상됨
> “We evaluated the proposed model with the task of statistical machine translation, where we used the RNN Encoder–Decoder to score each phrase pair in the phrase table.  
> The scores by the RNN Encoder–Decoder were found to improve the overall translation performance in terms of BLEU scores.”
> → phrase table의 각 구문쌍을 RNN Encoder–Decoder로 scoring 한 결과, BLEU 점수가 향상되었다.

## 한계점
1. 인코더가 긴 문장을 단일 벡터로 압축 → 정보 손실 발생
>“The encoder maps the whole input sequence into a fixed-length vector. … this may limit the performance when the input sequence is long.”


2. 완전한 Neural Translation 아님
3. 학습 비용, 데이터 제약 → 대규모 문장 단위 학습이 어려움
4. Attention 부재로 인한 장기 의존성 문제. RNN이 순차적으로 정보 처리 → 긴 문장에서 gradient vanishing 위험




%%
## 발표

RNN 인코더 디코더 - 시컨스 구조 반영 
인코더 - 입력 시퀀스 hidden state 업데이트 (고정 길이로 압축)
디코더 - 이전 생성된 단어 + hidden state 
빈도문 X , 언어의 구문적 성질 포착

긴 문맥 처리 + 학습 안정성
SMT → Blue 향상


### 질문
 - FNN 에서 왜 입력, 출력이 고정인지?
 → 임베딩 하는데, 당연히 고정. hidden state node:edge가 행렬. W가 정해져 있으니까, 더 크게 들어오면 가중치 행렬이 계산될 수 없음. 
 
- 긴 문장일때 정보 손실 없음?
- → 어느정도 있을 수 잇다..

- 빈도 정보를 학습하는 것이 아니고..
- → 상대적 빈도 정보를 학습함

- c 길이를 얼마로 해야할 것인가?
- 인풋 시퀀스가 너무 길면 세심한 정보를 잃어버리고, 너무 짧으면 불필요한 정보를 들어가지 않을까?
- → 디코딩할 때 fixed하진 않음.

- 학습 안정적이다 = gradient가 발산하지 않아야함.
- LSTM 에서는 파라미터 업데이트할 때, 더하기로 연결되어서 기울기 소실 없애줌. 메모리 측면에서도 효과적

%%