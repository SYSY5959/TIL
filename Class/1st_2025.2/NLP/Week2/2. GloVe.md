# GloVe: Global Vectors for Word Representation

*“GloVe는 단어 의미를 co-occurrence 확률의 비율(ratio)에 기반해 전역적 통계로 학습하는 log-bilinear 회귀 모델이다.”*

## 기존 방법론 한계

| 모델 유형                       | 대표 예                             | 장점                          | 한계                         |
| --------------------------- | -------------------------------- | --------------------------- | -------------------------- |
| **전역 행렬 분해 계열 (LSA, HAL)**  | LSA (Deerwester, 1990)           | 전체 말뭉치 통계를 잘 활용             | 빈번 단어에 영향 과대 / 선형 의미 구조 약함 |
| **로컬 윈도우 예측 계열 (Word2Vec)** | Skip-gram / CBOW (Mikolov, 2013) | 의미적 관계(king−man≈queen) 잘 포착 | 전체 통계 활용 비효율적 / 확률 근사      |
→ GloVe: 이 둘의 장점 결합

### (1) Global Matrix Factorization 계열 (LSA, HAL)
- 문서 전체를 보고 단어–단어(co-occurrence) 행렬을 만든 뒤, SVD로 차원 축소
- 전역 통계 활용이 효율적이지만, 의미적 방향성(analogical relationship)을 잘 포착하지 못함
	→ co-occurrence 빈도가 큰 단어(‘the’, ‘and’)가 유사도에 과도한 영향을 줌

> “A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure…”
> "...the most frequent words contribute a disproportionate amount to the similarity measure... even though they convey relatively little about semantic relatedness.”


### (2) Local Context Window 계열 (Skip-gram, CBOW)
- 문맥 기반으로 예측하며 의미적 구조를 잘 학습하지만, 전역 통계를 제대로 활용하지 못함.
	→ 동일 문맥이 여러 번 반복되어도 매번 별도 학습으로 처리됨 (비효율적)

> “...they scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.”


## GloVe 
“co-occurrence의 **전역 통계적 비율(ratio)** 이 단어 간 **의미 차이(선형 방향)** 를 가장 잘 설명한다.”

- **LSA의 전역 통계 효율성** + **Skip-gram의 선형 의미 구조**  
    → **GloVe의 log-bilinear 회귀식**

![](<./Images/2. GloVe_Figure.png>)
→ 절대 확률 값보다, **두 확률의 '비율'이 의미 차이**를 훨씬 잘 나타냄
- p(water|ice), p(water|stream) 둘 다 높은 확률 : 물과 관련 있다는 정보 뿐. 얼음과 증기 차이점 X
- 비율이 1보다 크면 ice쪽에, 1보다 작으면 steam쪽에 더 관련. 1 근처이면 둘 다 무관하거나 공통 관련
- → 비율이 크거나 작을수록 두 단어 사이의 의미적 구분(semantic distinction)이 명확해짐.

[영어로]
> - Both P(water∣ice)and P(water∣steam) are high, indicating relevance to _water_ but not distinguishing _ice_ from _steam_.
> - If the ratio P(k∣ice)/P(k∣steam)>1, the word is more related to _ice_; if < 1, more to _steam_; if ≈1, related to both or neither.
> - Thus, larger or smaller ratios capture clearer **semantic distinctions** between words.
> - ⇒ The **ratio of probabilities**, not their absolute values, better reveals differences in meaning.

[논문 원문]
> “Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid, gas) from irrelevant words (water, fashion).”
> → “절대 확률보다, 확률 비율이 관련 단어(solid, gas)와 비관련 단어(water, fashion)를 훨씬 더 잘 구분한다.”
> “The above argument suggests that the appropriate starting point for word vector learning should be with **ratios of co-occurrence probabilities rather than the probabilities themselves.**”
> → “단어 의미를 학습할 때는, 확률 자체가 아니라 **확률의 비율**로 시작해야 한다.”

### GloVe 수식
$X_{ik} = \dfrac{P(k|i)}{P(k|j)}​$ 의 로그가 아래 식으로 표현됨

- Ratio → Log → Dot-product
$$w_{i}^{T​} \tilde{w_{k}}​+b_{i}​+\tilde{b_{k}}​=logX_{ik}​$$
→ 선형 의미 구조 + 전역 통계 결합

[선형 의미 구조] LHS
- 선형 모델 → 단어의 의미를 벡터 공간에서의 방향과 크기로 표현 
- → 두 단어의 관계를 선형 연산으로 다룰 수 있음

[전역 통계] RHS
- 말뭉치 전체에서 단어들이 얼마나 자주 함께 등장했는가를 나타내는 co-occurrence 통계
- 전역 통계량 : $X_{ij}$ = 단어 i의 문맥(context) 안에서 j가 등장한 횟수
- $P_{ij}​ = P(j∣i) = \frac{X_{ij}}{X_{i}​}​$


- Cost Function
$$J=∑_{i,j}​f(X_{ij}​)(w_{i}^{T​} \tilde{w_{k}}​+b_{i}​+\tilde{b_{k}}​=logX_{ik})^2$$
$$
f(x) =
\begin{cases}
\left( x/{x_{\max}} \right)^{\alpha} & x < x_{\max} \\
1 & \text{otherwise}
\end{cases}
$$

> 실험에서는 $x_{\max} = 100, \ \alpha = 3/4$ 이 가장 안정적

→ 이 Cost Func으로 모델은 단어 의미의 선형관계를 말뭉치 전체의 통계로부터 자동으로 학습하게 됨

> “Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix... The model produces a vector space with meaningful substructure.”


### Skip-gram과 비교
- Skip-gram은 “로컬 윈도우에서 샘플링된 확률”을 이용해 확률 예측 손실을 최소화
- GloVe는 “전역 co-occurrence 카운트”를 이용해 로그 회귀 손실을 최소화
- **두 식은 결국 같은 목표(co-occurrence 확률 근사)를 다르게 최적화한 것**

🔹 **Word2Vec = 확률적(local) 접근**  
🔹 **GloVe = 회귀적(global) 접근**

### Complexity 
“co-occurrence 행렬 X을 다 써야 한다면, 계산량이 너무 크지 않나?”  
→ 저자들: “아니다. 실제로는 **희소성(sparsity)** 덕분에 Word2Vec보다도 빠르다.”

> “As can be seen from Eqn. (8)… the computational complexity depends on the number of nonzero elements in X…  
> the model scales no worse than $O(|V|^2)$…  
> but in practice we observe that $|X| = O(|C|^{0.8})$.”

실제로 한 문장에서 “멀리 떨어진 단어”는 거의 같이 등장하지 않기 때문에  
**co-occurrence 행렬은 매우 희소(sparse)** 


## 실험 및 결과

- **데이터셋:** Wikipedia, Gigaword, Common Crawl (최대 42B tokens)
- **평가:**
    - Word analogy (semantic/syntactic)
    - Word similarity (WordSim353, MC, RG 등)
    - NER (CoNLL-2003)
- **결과 요약:**
    - **Word analogy:** 42B 토큰에서 75% 정확도 (skip-gram 69.1%보다 우수)
    - **Word similarity:** 대부분의 벤치마크에서 GloVe > Word2Vec
    - **NER:** CRF 기반 태거에서 기존 모델 대비 1–2%p 향상
- **추가 분석:**
    - 차원 200 이상부터 수익 체감
    - 문맥창(window) 크기는 구문(syntax)엔 작게, 의미(semantics)엔 크게 유리
    - 학습 속도: 6B corpus에서 300차원 기준 14분/iteration (병렬 학습 가능)


## 결론 및 한계점
- **결론:**  
    GloVe는 _전역 통계의 효율적 활용_ 과 _의미 공간의 선형 구조 학습_ 을 결합한 모델이다.  
    Count-based와 Predictive 모델의 간극을 연결하는 이론적 기반을 제시했다.
- **한계점:**
    - Co-occurrence matrix 구축 비용이 여전히 큼 (|V|²의 희소 행렬)
	    → *FastText (subword로 일반화), BERT(행렬 직접 구성 없이 확률적 문맥 모델링)*
    - 문맥 순서 정보(order)를 반영하지 못함 
	    → *Transformer (Attention으로 순서, 문맥 관계 모두 학습)*
    - 한 단어는 하나의 벡터만 가짐 → 문맥에 따라 의미 변화 반영 불가 
	    → *ELMo (문맥적 임베딩)*
    - 희귀 단어, 문맥 다양성이 낮은 단어의 표현력 제한
    - 이후 contextual embedding (ELMo, BERT 등)으로 대체됨