# Enriching Word Vectors with Subword Information
TACL 2017

## 기존 방법론 한계
- Word2Vec, GloVe 같은 모델은 각 단어마다 고유한 벡터 부여
- → 형태 변화나 어근 관계 반영 X

> “Most of these techniques represent each word of the vocabulary by a distinct vector, without parameter sharing… they ignore the internal structure of words.”
> → 대부분의 기존 기법은 어휘 내 각 단어를 고유 벡터로 표현하며, 파라미터 공유를 하지 않는다.. 즉 단어 내부 구조를 무시한다. 


## FastText
💡 각 단어를 문자 n-gram 벡터들의 합으로 표현하자


> “Each word _w_ is represented as a bag of character n-grams.  
> We add special boundary symbols `<` and `>` at the beginning and end of words.”
> **We represent a word by the sum of the vector representations of its n-grams.**


1. 단어 양쪽에 특수 토큰 `<` `>`를 붙여 접두(prefix)와 접미(suffix) 구분
2. 단어 안에서 연속된 문자 n-gram 전부 추출 → *n_min ≤ n ≤ n_max (보통 3~6)*
3. 각 n-gram $g$에 벡터 $z_g$​를 학습. 한 단어의 벡터는 그 단어를 구성하는 n-gram 벡터의 합으로 정의
	$$u_{w​}=∑_{​g∈G_{w}​}z_g​$$
$$u_{unhappy}​=z_{<un}​+z_{unh​}+...+ z_{unhap}​+z_{happy}​+z_{appy}​+z_{ppy}​+z_{<unhappy>}$$
→ 'happy'의 의미가 'unhappy'에도 반영됨

- 모든 n-gram을 별도로 저장하면 메모리 폭발
- → FNV-1a 해시 함수 사용해서 n-gram을 고정된 크기 K의 해시 공간에 맵핑

### OOV (Out-of-Vocabulary) 단어 처리
- 기존의 Word2Vec은 사전에 없는 단어 → 벡터 없음
- FastText : n-gram 기반으로 OOV 단어도 쉽게 벡터 구성 가능
- ex) “bioinformatics”→ `bio`, `inform`, `matics` 벡터 합으로 추론 가능

> “We can also compute valid representations for out-of-vocabulary words by taking the sum of their n-gram vectors.”

### Ablation Study
- 어떤 단어가 중요한지 알 수 있음!!
- Negative sampling. 

| Ablation      | 실험 목표               | 결과                   | 시사점                       |
| ------------- | ------------------- | -------------------- | ------------------------- |
| **OOV 단어 처리** | subword 정보의 유효성     | sisg > sisg-         | OOV 표현 능력 입증              |
| **데이터 크기**    | 일반화 성능              | 소량 데이터에서도 유지         | 형태 기반 파라미터 공유 효과          |
| **n-gram 길이** | subword granularity | n=3~6 최적             | 짧거나 긴 n-gram은 비효율         |
| **언어 모델링**    | 실제 downstream 영향    | perplexity ↓         | subword 임베딩은 문법적 일반화에도 효과 |
| **형태소 모델 비교** | 기존 형태소 분석 기반 접근과 비교 | fastText가 단순하지만 더 강력 | 언어 비의존적 구조의 우월성           |

## 결론
*“Character n-gram 정보를 결합하면 간단하면서도 강력한 단어 임베딩을 얻을 수 있다.”*

- Word Similarity : 모든 언어에서 기존 Word2Vec보다 Spearman correlation ↑ (임베딩 공간이 사람이 인식하는 의미 구조와 더 가까워짐)
- Word Analogy : 문법적 유추(시제, 단복수.. )에서 큰 향상 → subword 구조 덕분에 문법 패턴 학습
- OOV 단어 : n-gram합으로 벡터 생성 가능
- LM : perplexity 8~13% 감소

> “We observe that morphological information significantly improves the syntactic tasks… improvement over the baselines is more important for morphologically rich languages.”
→ 형태 정보는 문법적 유추에 큰 개선 보이며, 형태소가 복잡한 언어에서 향상이 특히 크다

##  한계점
- n-gram의 순서·문맥(context) 정보는 여전히 무시됨 (bag-of-ngrams)
- 의미(semantic)보다는 **형태(syntactic) 개선**이 주효
- 너무 짧거나 긴 n-gram은 노이즈 발생
- 동음이의어나 문맥 의존 표현은 처리 불가 
	→ Contextual Embeddings (ELMo, BERT)
	- ELMo: contextual emb. 동음이의어 해결, 문맥 의존 표현
	- BERT: Subword + Context. Token-level contextual emb
	- GPT: subword → context → generative LM. 문맥 기반 확률 분포로 단어 생성