# Efficient Estimation of Word Representations in Vector Space

## 논문 정리
- **Word2Vec 모델(CBOW·Skip-gram)** 제안
- NLP 임베딩의 출발점: “단어를 벡터로 의미 공간에 위치시키는” 개념

*“Word2Vec은 기존의 복잡한 신경망 없이, 단순한 로그선형 모델로 효율적이고 확장 가능한 단어 의미 공간을 학습할 수 있음을 처음으로 입증했다.”*

### 주요 기여점

1. **CBOW (Continuous Bag-of-Words)**와 **Skip-gram** — 두 가지 새로운 **로그선형 모델(log-linear models)** 제안
    - CBOW: 주변 단어로 현재 단어를 예측
    - Skip-gram: 현재 단어로 주변 단어를 예측  
        이 두 모델은 기존의 신경망 언어모델(NNLM)보다 훨씬 효율적이면서도 의미적 관계 잘 포착

2. **단어의 벡터 연산적 특성 발견**
    - `vector("King") - vector("Man") + vector("Woman") ≈ vector("Queen")`  
        같은 **선형적 의미 관계(linear regularity)** 를 수학적으로 구현

3. **대규모 학습의 실현**
    - 구글 뉴스 6B 단어, 1M 단어 어휘로 훈련 → 단 하루 만에 고품질 벡터 생성 가능
    - 이전 NNLM보다 **수십 배 빠른 학습**

4. **의미적·통사적 관계 평가 데이터셋 제안**
    - **Semantic-Syntactic Word Relationship Test Set**을 새로 구축하여 다양한 관계(예: 국가-수도, 비교급, 복수형 등) 측정

### 한계점

1. **형태소 정보 부족** → “run”, “running”, “ran” 같은 변형어는 독립적으로 학습됨.
    (→ 이후 **FastText**가 서브워드(subword) 정보로 보완)
2. **문맥 의존성 부재** → 단어 의미가 문맥에 따라 달라지는 현상은 반영 불가.  
    (→ 이후 **ELMo**, **BERT** 등이 등장)
3. **고정 벡터** → 학습 후 업데이트가 불가능하며, 다의어 처리 어려움.

## Word2Vec 모델 구조

- 기존의 모델 NNML에서
	- 비선형 은닉층 제거 → Word2Vec은 **Log-linear Model**
	- 모든 단어가 하나의 projection layer를 공유하는 구조 
→ 효율 up!!

#### log-linear model
- Word2Vec의 벡터 공간이 선형적 구조
$$v_{king}​−v_{man}​+v_{woman}​ ≈ v_{queen}​$$

### CBOW
$$P(w_{t}​∣w_{t−N}​,…,w_{t−1}​,w_{t+1}​,…,w_{t+N}​)$$
- **주변 단어들(context words)** 을 보고, 그 **가운데 단어(target word)** 를 예측
- “문맥 → 단어” 방향의 예측

$$Q=N×D+D×log_{2​}(V)$$
- N: 문맥 단어 개수
- D: 임베딩 차원
- V: 어휘 수
- 👉 **은닉층이 없고 projection layer만 사용** (효율적)
> *the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix)*

#### 장점
- **빠름:** 입력 단어들을 평균내므로 계산이 간단
- **자주 등장하는 단어의 표현** 학습에 강함
- 문법적 패턴(통사적 관계)에 유리
#### 단점
- 희귀 단어(rare word)에 약함 — 중심 단어가 학습에 자주 등장하지 않으면 업데이트 빈도 낮음
- 문맥 단어 순서를 무시(Bag-of-Words)

### Skip-gram
$$P(w_{t-N}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+N} \mid w_t)$$
- **중심 단어(target word)** 로부터, 그 주변의 **문맥 단어들(context words)** 을 예측
- “단어 → 문맥” 방향의 예측

$$Q=C×(D+D×log_{2}​(V))
$$
- C: 최대 문맥 거리 (window size)
- D: 임베딩 차원
- V: 어휘 수
- 한 중심 단어로 주변 여러 단어를 예측하므로 C배 비용 듬

#### Skip-gram with Negative Sampling (SGNS) → FastText로!!
Skip-gram은 중심단어 $w_{I}$​가 주어졌을 때 주변 단어 $w_O$​가 등장할 확률을 최대화하는 모델
$$P(w_{O​}∣w_{I}​)= \frac{​​exp(u_{wO}​^{⊤}​v_{wI}​​)}{∑_{w=1}^{V}exp(u_{w}^{⊤}​v_{wI}​)}$$
$$L_{softmax}​=− logP(w_{O}​∣w_{I​})$$
→ 분모는 어휘 전체(V)에 대해 exp 연산 해야함. → 너무 느림

💡 모든 단어의 확률을 직접 계산하지 말고, *진짜 주변 단어인지 아닌지* 구분만 하자
→ **softmax 대신 ‘이진 분류(binary classification)’** 문제로 바꿈

$$\begin{equation}
\mathcal{L}_{\text{SGNS}} =
\log \sigma(\mathbf{u}_{w_O}^{\top} \mathbf{v}_{w_I})
+ \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)}
\left[ \log \sigma(-\mathbf{u}_{w_i}^{\top} \mathbf{v}_{w_I}) \right]
\end{equation}
$$
- k : negative sample 개수 (주변 단어로 나오지 않는 (가짜) 단어들 랜덤 샘플링)
- 앞에 항: 정답 주변 단어일 확률 ↑
- 뒤에 항: 랜덤(가짜) 단어일 확률 ↓

| 기존 Skip-gram (Softmax)       | SGNS (Negative Sampling) |
| ---------------------------- | ------------------------ |
| “중심 단어가 주변 모든 단어를 예측”        | “중심 단어가 주변 단어인지 아닌지만 판단” |
| 정답 주변 단어 확률 ↑, 나머지 ↓ (모두 계산) | 정답 1개 vs 랜덤 k개만 비교       |
| 분모: $\sum_{w=1}^{V}$ (전체 단어) | 샘플링된 k개만 계산              |
| 복잡도: (O(V))                  | 복잡도: (O(k)) (보통 5~15)    |

#### 장점
- **희귀 단어 학습에 강함:** 중심 단어 자체가 희귀해도 그 단어의 문맥 관계를 학습함
- **의미적 관계(semantic regularity)**를 더 잘 포착 (예: King - Man + Woman = Queen).
- 대규모 말뭉치에서도 뛰어난 일반화 성능

#### 단점
- 중심 단어마다 여러 예측을 해야 하므로 **CBOW보다 느림**
- 문법보다는 **의미 관계**를 더 잘 포착함 (하나의 단어 → 여러 문맥 단어 예측하니까)
