이미지를 입력으로 받아서 이를 분류하는 것
컴퓨터에게 이미지는 숫자들의 집합.
고양이 자세, 카메라 각도, 빛의 세기  등과 같은 작은 변화에 숫자 값들은 민감하게 반응. 이런 다양한 상황을 이겨낼 수 있는 "확장성"을 가진 이미지 분류 알고리즘 필요
## Data Driven Approch
1. Images와 labels의 데이터 셋 수집
2. Classifier 학습하기 위해 머신러닝 사용
3. 새로운 이미지를 classifier을 통해 판단

## Nearest Neighbor classifier
- 모든 데이터와 라벨들을 기억 (Training Images)
- 새로운 이미지를 Training image와 가장 비슷한 라벨 예측

### 코드 구현
train set 이미지 총 N개

train 함수: O(1)
test 함수: train 이미지와 모두 비교해야하니까 O(N)
→ Train time < Test time
실제로 우리가 원하는 건 train time 느려도 되지만, test time은 느리면 안됨.
NN 알고리즘은 이런 단점

CNN과 같은 parametric models은 반대로 train time 오래 걸리고, test time 적게 걸림

![[스크린샷 2025-01-24 오후 7.05.07.png]]


###  Distance Metric
#### L1 / L2 Distance
$L1 = \sum_{i=1}^{n} |x_i - y_i|$
→ 특징 벡터의 각각 요소들이 개별적인 의미(키, 몸무게,,,)를 가지면 L1 dist 

$L2 = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
→ 특징 벡터가 일반적인 벡터이고, 요소들간의 실질적인 의미를 모르는 경우 L2 dist
→ 좌표축(직교좌표계)에 영향을 받음

![[스크린샷 2025-01-24 오후 7.16.49.png]]
![[스크린샷 2025-01-24 오후 7.17.04.png]]
→ 어떤 거리 척도를 쓰는지에 따라 결정 경계의 모양이 달라질 수 있음

## k - Nearest Neighbor Classifier
NN의 일반화 버전인  k-NN 알고리즘
Distance metric 이용해 가까운 이웃을 k개 만큼 찾고, 이웃끼리 투표하는 방식.
가장 많은 득표수를 획득한  레이블로 예측

k = 1일 때, train set의 정확도 최고. But, test set에서는 성능 안좋음
k = 5일 때, train set에서 정확도가 최고가 아니더라도, test set에 대해서는 성능 좋음

![[스크린샷 2025-01-24 오후 7.11.17.png]]

## Setting Hyperparameters
Idea #2: test set에서만 잘 작동하는 하이퍼  파라미터 고른 것 → 한번도 보지 못한  데이터에서의 성능을 대표X
![[스크린샷 2025-01-23 오전 1.19.36.png]]
→ BEST : Train set으로 모델 학습. Validation set의 성능이 최대인 하이퍼파라미터 선택. 그리고 마지막에 test set으로 한번만 수행해서 평가.


🧐 Train set vs Validation set ?
>Train : 레이블 볼 수 있음
>Validation : 레이블 볼 수 없음. 레이블은 알고리즘이 얼마나 잘 작동하는지 확인할 때만 사용

🧐 Test set 이 한번도 보지 못한 데이터를 대표할 수 있는지?
>데이터는 독립적이며 유일한 하나의 분포에서 나온다는 가정 (iid assumption)
>모든 데이터는 동일한 분포를 따른다고 생각해야 함
>데이터를 수집할 때, 일관된 방법론을 가지고 대량의 데이터 한번에 수집하는 전략. 그 다음 무작위로 train, test 나눔
>데이터가 지속적으로 업데이트 되는 경우, 데이터셋 전제 무작위로 섞어서 나누기

### Cross - Validation
→ 적은 데이터일 때. 딥러닝에서는 많이 쓰이지 X (학습 계산량 ⬆️)
- fold 1,2,3,4로 하이퍼 파라미터 학습하고, fold 5로 알고리즘 평가
- fold 1,2,3,5로 .. , fold 4로 알고리즘 평가

![[스크린샷 2025-01-23 오전 1.13.52.png]]


### 이미지 분류에서는 KNN 사용 X
why?
#### 1. knn 너무 느림. (train 학습 시간 < test 적용 시간)
#### 2. L1/L1 Distance : 이미지 간의 거리 측정에 적절 X 
![[스크린샷 2025-01-23 오후 3.52.51.png]]
→ 오른쪽 3개의 이미지는 왼쪽 이미지와 모두 같은 L2 Distance
→ 이미지들간의 지각적 perceptual유사도 측정 X
#### 3. 차원의 저주 (Curse of Dimensionality)
KNN : Train set 이용해서 공간 분할
knn 잘 작동하려면 전체 공간을 조밀하게 커버할 만큼의 충분한 train sample 필요
→ 차원이 증가할수록 필요한 학습 데이터 기하급수적으로 증가
→ 고차원의 이미지를 모든 공간을 메울만큼 데이터를 모으는 것은 현실적으로 불가능
![[스크린샷 2025-01-23 오후 3.59.23.png]]

## KNN 정리
- Image classification : Training set(images + labels), Test set(predict labels)
- KNN classifier : 근처 training examples에 기반하여 labels 예측
- hyperparameters: Distance metric, K → validation set으로 결정. 
- 샘플들의 manifolds 가정 X → KNN 제대로 작동하려면 공간 덮는 충분히 많은 training set samples를 가져야함

### 추가) Manifold Assumption
"데이터가 고차원 공간에 있지만, 내재적으로 저차원 구조를 따른다."
데이터가 단순히 무작위로 공간에 흩어진 것이 아니라, 내재적으로 낮은 차원의 구조를 따른다.
KNN : Manifold 가정 따르지 않고, 단순히 데이터 간의 거리를 비교해 작동

[manifold 가정 알고리즘]
 >Mainfold Learning: t-SNE, UMAP 
 >비선형 분류 알고리즘: SVM 

## Linear Classification
다양한 종류의 딥러닝 알고리즘들의 가장 기본이 되는 블럭
"parametric model"의 가장 단순한 형태
### parametric model
- 입력 X (Image 32x32x3)
- parameter W 


KNN : parameter X → 모든 train set을 test time에 사용
parametric model: train set의 정보를 요약하여 W에 모아줌. → test time에 W만 있으면 됨. train set 필요 X
→ 작은 디바이스에서 모델 동작시켜야할 때 효율적
→ 딥러닝은 함수 f의 구조를 적절하게 잘 설계하는 일
![[스크린샷 2025-01-23 오후 4.47.23.png]]
$$F(x,W) = Wx$$
- 입력 이미지 x : 32x32x3 = 3072 dim
- 출력 10개 클래스 스코어 f(x,W) : 10x1
- 파라미터 W : 10x3072
- Bias : 10x1 → 입력과 무관. 독립적

![[스크린샷 2025-01-23 오후 5.02.21.png]]
- input image: 2x2 =4 픽셀
- 가중치 W : 4x3
- 출력 : 3x1
- bias : 3x1 → scaling offsets 더해줌
-  W의 행벡터와 x의 열벡터 내적 : 클래스 간 템플릿의 유사도 측정

### CIFAR - 10
10개 카테고리의 행 벡터 시각화
![[스크린샷 2025-01-23 오후 5.02.06.png]]
→ 하나의 클래스에 다양한 특징들이 존재할 수 있는데, 모든 것을 평균화 시킴. 각 카테고리 인식하기 위한 템플릿 단 하나만 존재 (문제점)

![[스크린샷 2025-01-23 오후 5.18.14.png]]
- 각 이미지를 고차원 공간의 한 점이라고 생각
- Linear classifier은 각 클래스를 구분하는 선형 결정 경계 (Decision Boundary)
- 고차원 공간(이미지의 픽셀 값)에서의 결정 경계 학습


### Linear Classification 가 풀기 어려운 문제
1. 반전성 문제 (Parity problem) - 이미지 속 사람 수 홀/짝수 분류 문제
	0보다 큰 픽셀 수 홀/짝에 따라 분류
	ex. (1,-1) : 1개 → 2사분면
3. Multimodal Distribution
	multimodal distribution이면, 한 클래스가 다양한 공간에 분포할 수 있음
	ex. 이전 말의 머리가 2개인 이미지
![[스크린샷 2025-01-23 오후 5.20.27.png]]


#### Multimodal Distribution
단일 모달리티 내에서 데이터가 여러 밀집된 영역을 가지는 경우
ex. 이전 말의 머리가 2개인 이미지
이미지 자체는 하나의 모달리티로 구성. 말 머리가 2개로 보이면, 시각적  정보의 구조적 복잡성. 애매한 패턴 때문에 혼란스러워질 가능성 높음. 이 경우엔, 비정상적인 패턴을 이해하여 올바르게 분류할 수 있어야함

#### 추가) Multimodal Problem
단일 모달리티(텍스트만, 이미지 하나만)를 처리하는 것과 달리, 여러 모달리티를 결합하여 정보 간의 관계 이해하고 처리한다는 점에서 더 복잡함
