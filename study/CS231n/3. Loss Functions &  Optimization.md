
## Loss function
모델이 얼마나 잘 학습하고 있는지 평가하는 것. 예측값과 실제값 간의 차이 수량화
Image classification : CIFAR-10의 10개 카테고리 중 하나로 분류하는 것. 여기서 y는 10개가 됨(1~10)

$x_i$ : image
$y_{i}$ : (integer) label
$$L = \frac{1}{N} \sum_i L_i(f(x_i, W), y_i)
$$

이 Loss function을 최소화하는 W 찾기

## Multi-class SVM loss
$s = f(x_{i}, W)$ : score vector
$S_{y_{i}}$ : train set의 i번째 이미지의 **정답 클래스 스코어**

Loss : 정답보다 다른 클래스의 점수가 높을 때, 그 차이 (0보다 작은 음수 포함 X)
이 Loss에 safety margin 값 추가 → 정답 클래스가 적어도 다른 클래스보다 safety margin 값 만큼은 커야 한다는 이야기. 
![[스크린샷 2025-01-26 오후 3.00.17.png|300]]
- 여기서 `safety margin = 1`
- 가로축 : $S_{y_{i}}$ , 세로축 : $L_{i}$ (loss)]
- 최소: 0 , 최대: 무한대

### Loss 계산 예제
![[스크린샷 2025-01-26 오후 3.11.21.png]]
#### Cat
- Loss(class: `car`) = max(0, 5.1 - 3.2 + 1) = 2.9
- Loss(class: `frog`) = max(0, -1.7 - 3.2 + 1) = 0
- 고양이 사진의 전체 Loss = 2.9 + 0 = 2.9

#### Car
- Loss(class: `cat`) = max(0, 1.3 - 4.9 + 1) = 0
- Loss(class: `frog`) = max(0, 2.0 - 4.9 + 1) = 0
- 자동차 사진의 전체 Loss = 0

#### Frog
- Loss(class: `cat`) = max(0, 2.2 +3.1 + 1) = 6.3
- Loss(class: `car`) = max(0, 2.5 +3.1 + 1) = 6.6
- 개구리 사진의 전체 Loss = 6.3 + 6.6 = 12.9

$$L = \frac{1}{N} \sum_{i=1}^{N} L_i
$$
→ 최종적인  Loss = (2.9 + 0 + 12.9) / 3 = 5.27

### Multi-class SVM loss 특징
- Multi-class SVM loss에서 스코어 자체에 대한 해석 X
- 단지, 정답 클래스가 정답 아닌 클래스들 보다 더높은 스코어 내기만 원함

❓파라미터를 가장 작은 수로 초기화 하면, 처음 학습 시에는 결과 스코어가 임의의 일정한 값을 갖게 됨.
만약 모든 스코어가 거의  0에 가깝고, 값이 서로 비슷하다면?
Multiclass SVM에서 `loss = C - 1` 
(C : 클래스 수, 자기 자신은 loss = 0이고, 나머지 C-1개 클래스는 1이니까..) 
→ 디버깅 전략으로 유용 (train 처음 시작할 때 loss 가 C-1이 아니면 버그 있는 것!!!)

❓ loss func에서 전체합이 아닌 평균을 쓴다면?
→ 답에 영향 X. 단지 스케일만 변할 뿐. 스코어 값이 몇인지는 중요하지 않음

❓ loss func에서 제곱항이 되면?
*Squared Hinge Loss*
$$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)^2
$$
→ good & bad 사이의 trade off를 비선형적인 방식으로 바꿔줌

❓loss = 0 인 W가 유일?
→ 유일 X

#### Hinge Loss
$$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)
$$

#### Code
```python
def L_i_vectorized(x, y, W):
    scores = W.dot(x)
    margins = np.maximum(0, scores - scores[y] + 1)
    margins[y] = 0 
    loss_i = np.sum(margins)
    return loss_i
```

 여기서, max로 나온 결과에서 정답 클래스만 0으로 만들어 줌
 → 전체 순회할 필요가 없는 일종의 vectorized 기법

## Regularization
- 모델의 복잡함과 단순함 통제
- 과적합을 방지하기 위해 Regularization term 추가 → 모델이 좀 더 단순한 W를 선택하도록 도와줌
- 머신러닝에서는, Regularization penalty
![[스크린샷 2025-01-26 오후 4.21.44.png]]

### L1 / L2 Regularization
- L1 Regularization : 절댓값 합 
→ W가 0으로 수렴하는 것이 많음. Sparse(드문) solution

- L2 Regularization : 제곱합 
→ 큰 W의 값을 줄이며, 대부분 값들이 0에 가까운 값을 가지는 가우시안 분포 가짐. 아예 0이 되는 것은  아님. x의 모든 요소가 영향을 줌.

#### 추가) MAP 추론
[베이즈 정리]
$$P(w \mid D) \propto P(D \mid w) \cdot P(w)$$
[MAP 추론]
$$w_{\text{MAP}} = \arg\max_w \left[ \log P(D \mid w) + \log P(w) \right]$$

$$\text{Loss: } \text{MSE} + \lambda \sum w_i^2$$


[Gaussian Prior]
$$P(w) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{w^2}{2\sigma^2}\right)$$
$$\log P(w) = -\frac{w^2}{2\sigma^2} + \text{(상수항)}$$
$$\lambda \propto \frac{1}{\sigma^2}$$

- L2 Regularization을 사용하면, 이는 **Bayesian 관점**에서 파라미터 W에 대해 **Gaussian prior**를 가정하고, MAP 추론을 수행하는 것과 동일
- 즉, L2 Regularization은 파라미터가 0을 중심으로 한 정규 분포(가우시안)에 가까워지도록 제약을 거는 과정으로 볼 수 있다는 뜻

### 다른 Regularization 방법
- Elastic net Regularization : L1 + L2
- Max norm regularization 
- Dropout
- Batch Normalization, Stochastic depth

## Softmax Classifier 
- Multi-class SVM loss에서 스코어 자체에 대한 해석 X
- Multinomial Logistic regression loss에서는 스코어 자체에 추가적인 의미 부여
→ 여기서 softmax 함수 씀

*k -번째 클래스에 대한 확률* 
$s = f(x_i; W)$

$$P(Y = k \mid X = x_i) = \frac{e^{s_k}}{\sum_{j} e^{s_j}}$$
*Loss function*
$$L_i = -\log P(Y = y_i \mid X = x_i)$$
$$L_i = -\log\left(\frac{e^{s_{y_i}}}{\sum_{j} e^{s_j}}\right)$$


![[Pasted image 20250127200421.png]]→ 원래 Score에 지수함수 exp 하고, 정규화하고, 거기에 -log 하면 loss값이 됨.

❓Loss func의 최대, 최소는?
→ 최소는 0, 최대는 무한대
loss =0 or 1이 되려면 실제 스코어는 극단적으로 높거나 낮아야함 (+- 무한대).

❓Score이 모두 0 근처의 작은 수 일 때 loss는?
→ - log(1/C) = log(C)
→ 디버깅 전략: Softmax 사용할 때, 첫번째 iteration에서 loss = log(C)가 아니면 잘못된 것!!

## Multi-class SVM vs Softmax
[SVM]
- Hinge Loss
- 정답 score, 정답 아닌 score 간의 **margin에만 집중**
- Score 조금 변한다고, SVM loss 변화 X
→ 일정 margin을 넘기면 더 이상 성능 개선 X

[Softmax]
- Cross - entropy Loss
- 확률을 구해서 -log(정답 클래스)에 집중
- 정답 클래스에 확률을 최대한 높이려고 함 
→ 더더더ㅓㅓ 성능 개선하려고 함!! 

## Optimization
### Gradient
- 수치적인 Numeric gradient 느림. 부정확함. 실제로 사용 X
- 해석적인 Analytic gradient 계산 빠름
- Numeric gradient 이용한 디버깅 전략: 파라미터 스케일 줄이기 (디버깅 오래 안걸림)


```python
# Vanilla Gradient Descent

while True:
    weights_grad = evaluate_gradient(loss_fun, data, weights)
    weights += - step_size * weights_grad  # perform parameter update
```
- 가중치를 gradient의 반대 방향으로 업데이트
- step size : Learning rate → 가장 먼저 정해주는 하이퍼파라미터
- 전체 loss = train set loss의 평균 → N이 크다면 계산 엄청 느려짐
- loss는 loss의 gradient의 합 → N개 전체 train set을 돌면서 계산

### Stochastic gradient descent
-  거의 모든 DNN 알고리즘에 사용되는 기본적인 학습 알고리즘
- 전체 데이터 셋의 gradient와 loss 계산하기 보다, Minibatch 라는 작은 training sample 집합으로 나누어서 학습
- Minibatch는 보통 2의 거듭제곱으로  정함. 주로 32, 64, 128
- 이 작은 minibatch를 이용해서 loss의 전체 합 추정치와 실제 gradient의 추정치 계산.

```python
# Vanilla Minibatch Gradient Descent

while True:
    data_batch = sample_training_data(data, 256)  # sample 256 ex
    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += - step_size * weights_grad  # perform parameter update
```

## 특징 변환
- 이미지 분류에서 Linear Classification은 좋은 방법 X  
→ DNN 유행 전에는 2가지 stage 거치고  L.C 씀
1. 이미지의 여러가지 특징 표현 계산
	모양새, 컬러 히스토그램, edge 형태와 같은 특징 표현을 연결한 특징 벡터
2. 이 특징벡터를 Linear Classification에 입력값으로 사용
- 이렇체 추출된 feature extractor는 Classifier를 학습하는 동안 변하지 X
- 학습할 때는 Linear Classifier만 업데이트
- CNN, DNN도 마찬가지. 다른점은 이미 만들어놓은 특징 벡터를 쓰는 것이 아닌 **데이터로부터 특징들을 직접 학습**하려고 함.

### 컬러 히스토그램
- 이미지 분석할 때 색상 분포를 표현하는 방법
- 각 이미지의 픽셀이 가지는 색상(Hue)을 기준으로 히스토그램을 나타내어 특징 추출에 사용

![[스크린샷 2025-01-27 오후 10.44.22.png]]

### Histogram of Gradient
- 이미지에서 형태와 객체의 구조적 특징을 추출하기 위해 사용되는 기술
- edge 방향과 크기를 기반으로 히스토그램을 생성하여 이미지를 특징 벡터로 변환 
![[스크린샷 2025-01-27 오후 10.49.55.png]]

### Bag of Word
- 자연어처리에서, BoW는 문장의  여러 단어의 발생 빈도를 세서, 특징 벡터로 사용하는 것
- BoW를 이미지에 적용. 이미지에서 visual words를 정의하기 위해, 이미지 임의로 조각내고, K-means과 같은 알고리즘으로 군집화.
- 다양하게 구성된 각 군집들은 다양한 색과 다양한 방향에 대한 edge도 포착할 수 있음
![[스크린샷 2025-01-27 오후 10.57.35.png]]