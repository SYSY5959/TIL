## Computational Graphs
수학적 연산을 그래프 구조로 표현한 것.
input, local gradient 쉽게 확인 가능. 복잡한 형태의 Analytic gradient 쉽게 계산할 수 있음
→ 그래프 기반으로 병렬 연산  가능하여 연산 속도 최적화. 메모리 효율성

![[스크린샷 2025-02-03 오후 11.31.59.png]]

## Forward Propagation
데이터 흐름에 따라 결과(loss)를 계산하는 과정

## Backward Propagation
forward prop 끝난 이후에, Chain Rule을 이용하여 Gradient 계산하는 과정
복잡한 함수의 gradient 구하기 → chain rule 이용해서 구하면 쉬움

![[스크린샷 2025-02-03 오후 11.36.44.png]]
여기서 $\frac{df}{dx}= -4$ 인 이유는 -4 (z) 곱하기 1


### local gradient VS gradient
![[스크린샷 2025-02-04 오전 11.47.31.png]]
local gradient : 하나의 노드 안에 있는 함수의 gradient. 입력에 대한 출력의 gradient
gradient : 최종 출력 L에 대한 특정 변수의 미분. Chain Rule 이용해서 local gradient들을 곱해서 계산
upstream gradient : 뒤에 있는 노드에서 오는 gradient

`Gradient = Upstream Gradient × Local Gradient`

| 개념                    | 의미                          | 예제 (\( z = (x+y) \times w \))                                          |
| --------------------- | --------------------------- | ---------------------------------------------------------------------- |
| **Local Gradient**    | 한 연산 노드에서 입력에 대한 출력의 변화율    | $\frac{\partial u}{\partial x} = 1, \frac{\partial z}{\partial u} = w$ |
| **Upstream Gradient** | 이전 노드에서 현재 노드로 전달된 Gradient | $\frac{\partial z}{\partial u} = w$                                    |
| **Gradient**          | 최종 출력 \( z \) 에 대한 변수의 변화율  | $\frac{\partial z}{\partial x} = w \times 1 = w$                       |

![[스크린샷 2025-02-04 오후 12.44.38.png]]
$\frac{dL}{dx0}= 2 \times 0.2 = 0.4$   local gradient = 2 (W0) , upstream gradient =0.2

### Sigmoid
입력 x를 0과 1 사이의 값으로 변환하는 비선형 활성화 함수
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
$$
\sigma'(x) = \sigma(x) (1 - \sigma(x))
$$
![[스크린샷 2025-02-04 오후 12.45.03.png]]

### Patterns in backward flow
![[스크린샷 2025-02-04 오후 12.49.37.png]]
- `add gate` : 이미 가지고 있던 gradient를 각각의 노드의 `분배`
- `max gate` : `더 큰 쪽에만 gradient를 전달`하고 `작은 쪽은 0값`
- `mul gate` : 현재의 gradient를 `각각 숫자에 곱해서 바꿔치기` 

### Gradient for vectorized code
앞에서 입력값은 Scaler. 실제 입력값은 vector.
#### Jacobian matrix
`다변수 벡터 함수(vector-valued function of multiple variables)` 에 대한 일차미분값 
jacobian 행렬은 항상 input 행렬과 같은 크기

![[Pasted image 20250204131114.png]]

![[Pasted image 20250204130939.png]]

## Neural Networks

Linear Classifier 2개 이상 쌓아올리는 형태.  이 사이에 Non-linear func 사용해야함

![[스크린샷 2025-02-04 오후 1.13.13.png]]
3072개의 입력 x와 W1과 linear classification 결과로 100개의 h 값을 계산 → 이 값에 Non-linear max 함수를 취한 뒤에 W2값을 linear classification으로 사용하여 10개의 결과 s 가지는 것

선형 레이어 하나만 쌓으면 빨간색 자동차만  찾음. 
Neural Network 이용하면 빨간색, 노란색 다양한  색깔 찾을 수 있음
→ 레이어를 쌓아가면서  여러 특징  추출할 수 있음

#### 왜 NN은 다양한 색상을 찾을 수 있을까?
여러 개의 비선형 변환(다층 레이어)을 적용하여 **더 복잡한 특징(feature)을 추출**할 수 있도록 설계
1. **여러 층**을 거쳐 점점 더 복잡한 특징을 학습함.
2. 선형 분류기와 달리, NN은 레이어마다 **비선형 활성화 함수**를 사용하여 복잡한 결합 관계를 학습할 수 있음
3. 다양한 특징 학습 가능 → 더 일반화된 분류  가능

#### 비선형 활성화 함수 종류
![[스크린샷 2025-02-04 오후 1.22.49.png]]

### NN 구조
![[스크린샷 2025-02-04 오후 1.22.01.png]]